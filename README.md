![Python](https://img.shields.io/badge/Python-3.9+-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-Deep%20Learning-orange)
![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-green)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)
![YOLOv8](https://img.shields.io/badge/YOLOv8-Object%20Detection-purple)

# ğŸ¯ Resume Section: Projects

**Having Fun with Computer Vision**
**Python, TensorFlow, OpenCV, Streamlit, YOLOv8, MediaPipe**

* Built a multi-feature computer vision system integrating emotion recognition, hand tracking, gesture-based controls, and real-time object detection.

* Designed and trained a MobileNet-based CNN achieving accurate 7-class facial emotion classification.

* Developed a centralized Streamlit dashboard to launch multiple real-time vision modules.

* Implemented gesture-based volume control using MediaPipe Hands and system-level audio APIs.

* Integrated YOLOv8 for real-time object detection with webcam inference.

* Engineered modular, portable codebase suitable for cross-platform deployment.



# ğŸŒŸ Features
## ğŸ˜Š Emotion Detection

**Real-time facial emotion recognition using a MobileNet-based CNN**

**Trained on a Kaggle facial emotion dataset** (https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer)

Detects 7 emotions:

* Angry

* Disgust

* Fear

* Happy

* Neutral

* Sad

* Surprise

## âœ‹ Hand Tracking

**Real-time hand landmark detection using MediaPipe Hands**

* Finger counting and thumb direction detection

* Smooth and stable tracking

## ğŸ”Š Volume Gesture Control

**Control system volume using thumbâ€“index finger distance**

**Built with MediaPipe + Pycaw (Windows)**

**Visual feedback with dynamic volume bar**

## ğŸ® Virtual Games (Gesture-Controlled)

**Interactive games controlled entirely using hand gestures:**

### Guessing Game â€“ virtual keyboard interaction

### Psychology Test â€“ gesture-based personality test

### Tic-Tac-Toe â€“ play against the computer using hand movements

## ğŸ” YOLOv8 Object Detection & Tracking

**Real-time object detection using YOLOv8 (Ultralytics)**

* Webcam-based inference

* Automatic model download and recovery

* Bounding boxes with class labels

## ğŸ§  Emotion Detection Model Details

**Backbone: MobileNet (pretrained on ImageNet)**

### Custom Head:

* Global Average Pooling

* Dense (512, ReLU)

* Dropout (0.5)

* Softmax (7 classes)

* Optimizer: Adam (learning rate = 1e-4)

* Loss Function: Categorical Crossentropy

# ğŸ“ Project Structure

```
Having-Fun-with-Computer-Vision/
â”œâ”€â”€ Emotion_detection/
â”‚   â”œâ”€â”€ em_de.ipynb
â”‚   â”œâ”€â”€ emotion_webcam.py
â”‚   â””â”€â”€ best_model.h5
â”‚
â”œâ”€â”€ pose_detection/
â”‚   â””â”€â”€ hand_tracking.py
â”‚
â”œâ”€â”€ virtuals/
â”‚   â”œâ”€â”€ guessing_game.py
â”‚   â”œâ”€â”€ psychology_test.py
â”‚   â”œâ”€â”€ tic_tac_toe.py
â”‚   â””â”€â”€ volume_gesture_control.py
â”‚
â”œâ”€â”€ yolo_webcam_detection/
â”‚   â””â”€â”€ Tracking.py
â”‚
â”œâ”€â”€ cv_project_ui.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
# ğŸ“Š Dataset

**Source: Kaggle Facial Emotion Dataset(https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer)**

Images resized to **224 Ã— 224**

Organized into
train/ and test/ folders

**âš ï¸ The dataset is not included in this repository.**

Expected structure:
```
Emotion_detection/
â”œâ”€â”€ train/
â””â”€â”€ test/
```
# âš™ï¸ Installation
```
git clone https://github.com/your-username/Having-Fun-with-Computer-Vision.git
cd Having-Fun-with-Computer-Vision
pip install -r requirements.txt
```
> âš ï¸ Python 3.9+ is recommended.

# â–¶ï¸ Run the Main Application (Recommended)
```
streamlit run cv_project_ui.py
```
This launches a central dashboard from which all features can be started.
# â–¶ï¸ Run Individual Modules (Optional)
```
python Emotion_detection/emotion_webcam.py
python pose_detection/hand_tracking.py
python virtuals/volume_gesture_control.py
python yolo_webcam_detection/Tracking.py
```
# ğŸ§ª Technologies Used

* Python

* TensorFlow / Keras

* OpenCV

* MediaPipe

* Streamlit

* YOLOv8 (Ultralytics)

* NumPy, Pandas, Matplotlib

# ğŸš€ Future Enhancements

* Cross-platform volume gesture support

* Face alignment for improved emotion recognition

* Dockerized deployment

* Web and mobile deployment

* Performance optimization for low-end devices

# ğŸ‘©â€ğŸ’» Author

**Mariam Ghareeb**
**Computer Vision & Deep Learning Enthusiast**

#  License

**This project is licensed under the MIT License.**

# â­ If you find this project useful

**Please consider starring â­ the repository â€” it helps others discover the project!**
